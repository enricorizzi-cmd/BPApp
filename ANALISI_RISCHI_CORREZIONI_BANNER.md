# ‚ö†Ô∏è ANALISI RISCHI E CONTROINDICAZIONI - CORREZIONI BANNER E NOTIFICHE

**Data**: 2025-11-10  
**Scope**: Analisi completa rischi e controindicazioni delle correzioni proposte

---

## üìä **RIEPILOGO ESECUTIVO**

**Livello Rischio Complessivo**: üü° **MEDIO-ALTO**

**Raccomandazione**: 
- ‚úÖ **FASE 1 (Correzioni Critiche)**: Procedere con cautela, test approfonditi
- ‚ö†Ô∏è **FASE 2 (Miglioramenti)**: Valutare impatto prima di implementare
- üî¥ **FASE 3 (Validazione)**: Obbligatoria prima di produzione

---

## üî¥ **RISCHI CRITICI**

### **1. RISCHIO: DUPLICATI NOTIFICHE E BANNER** üî¥ ALTO

**Causa Root:**
- Backend e frontend possono processare lo stesso appuntamento
- Race condition tra job backend (7 min) e scan frontend (5 min)
- Doppio trigger: push notification + banner frontend

**Scenario Critico:**
```
T0: Appuntamento finisce
T1: Backend job trova appuntamento ‚Üí invia push ‚Üí marca come sent
T2: Frontend scan trova stesso appuntamento ‚Üí mostra banner ‚Üí invia push (duplicato!)
```

**Impatto:**
- ‚ùå Utente riceve 2 push notification per stesso appuntamento
- ‚ùå Banner appare anche se push gi√† inviata
- ‚ùå Confusione utente
- ‚ùå Spam notifiche

**Probabilit√†**: üü° **MEDIA** (50-70%)
**Severit√†**: üî¥ **ALTA** (UX degradata, spam)

**Mitigazione:**
1. ‚úÖ Frontend controlla `pushSent()` PRIMA di `triggerPush()`
2. ‚úÖ Backend marca come sent PRIMA di inviare
3. ‚ö†Ô∏è **PROBLEMA**: Race condition se entrambi eseguono simultaneamente
4. üîß **SOLUZIONE**: Frontend deve controllare push tracking PRIMA di mostrare banner

**Raccomandazione**: 
- ‚ö†Ô∏è **CRITICO**: Aggiungere check `pushSent()` nel frontend PRIMA di `enqueueBanner()`
- ‚ö†Ô∏è **CRITICO**: Aggiungere lock/distributed lock per evitare race condition

---

### **2. RISCHIO: PERFORMANCE DATABASE** üî¥ ALTO

**Causa Root:**
- Query estesa da 2 ore a 7 giorni = **84x pi√π dati potenziali**
- Scan frontend ogni 5 minuti = **288 query/giorno per utente**
- Nessun caching o dedupe

**Calcolo Impatto:**
```
Scenario Attuale:
- Query backend: 7 giorni √ó ~10-20 appuntamenti/giorno = 70-140 appuntamenti
- Query frontend: FULL scan tutti appuntamenti utente (potenzialmente 100+)

Scenario Con Correzione:
- Query backend: 7 giorni √ó ~10-20 appuntamenti/giorno = 70-140 appuntamenti ‚úÖ (stesso)
- Query frontend: FULL scan ogni 5 minuti = 288 query/giorno √ó N utenti
```

**Impatto:**
- ‚ùå Carico database Supabase aumentato del 200-500%
- ‚ùå Possibile throttling Supabase (rate limits)
- ‚ùå Latenza query aumentata
- ‚ùå Costi Supabase potenzialmente aumentati

**Probabilit√†**: üü° **MEDIA** (40-60%)
**Severit√†**: üî¥ **ALTA** (degradazione performance, costi)

**Mitigazione:**
1. ‚úÖ Backend gi√† ha limite 100 appuntamenti
2. ‚úÖ Batch processing gi√† implementato
3. ‚ö†Ô∏è **PROBLEMA**: Frontend non ha caching
4. üîß **SOLUZIONE**: 
   - Aggiungere cache frontend (5 minuti)
   - Incremental refresh invece di full scan
   - Debounce scan se utente non attivo

**Raccomandazione**:
- ‚ö†Ô∏è **CRITICO**: Implementare caching frontend PRIMA di riabilitare scan periodico
- ‚ö†Ô∏è **IMPORTANTE**: Monitorare metriche Supabase dopo deploy

---

### **3. RISCHIO: MEMORY LEAK FRONTEND** üü° MEDIO-ALTO

**Causa Root:**
- Scan periodico ogni 5 minuti
- `_pending` Set non viene mai pulito completamente
- Event listeners multipli se pagina ricaricata

**Scenario Critico:**
```
T0: Utente apre pagina ‚Üí scan() ‚Üí setInterval registrato
T1: Utente naviga (SPA) ‚Üí pagina non ricaricata ‚Üí nuovo setInterval
T2: Dopo 1 ora = 12 setInterval attivi ‚Üí 12 scan simultanei ogni 5 min
```

**Impatto:**
- ‚ùå Memory leak progressivo
- ‚ùå CPU spike ogni 5 minuti
- ‚ùå Browser crash su dispositivi low-end
- ‚ùå Battery drain mobile

**Probabilit√†**: üü° **MEDIA** (30-50%)
**Severit√†**: üü° **MEDIA** (degradazione performance client)

**Mitigazione:**
1. ‚úÖ `_pending` Set ha auto-clear (5 minuti)
2. ‚ö†Ô∏è **PROBLEMA**: setInterval non viene cleanup su navigazione SPA
3. üîß **SOLUZIONE**: 
   - Cleanup setInterval su `beforeunload` / `visibilitychange`
   - Singleton pattern per scan
   - Debounce scan se gi√† in esecuzione

**Raccomandazione**:
- ‚ö†Ô∏è **IMPORTANTE**: Implementare cleanup listeners PRIMA di riabilitare scan
- ‚ö†Ô∏è **IMPORTANTE**: Test su mobile devices

---

### **4. RISCHIO: NOTIFICHE SPAM UTENTE** üü° MEDIO

**Causa Root:**
- Backend processa appuntamenti vecchi (fino a 7 giorni)
- Se utente non ha risposto banner, riceve notifica ogni 7 minuti
- Nessun rate limiting per utente

**Scenario Critico:**
```
T0: Appuntamento finisce 6 giorni fa
T1: Backend trova appuntamento ‚Üí invia push
T2: Utente non risponde (banner chiuso, push ignorata)
T3: 7 minuti dopo ‚Üí backend trova stesso appuntamento ‚Üí invia push (duplicato!)
```

**Impatto:**
- ‚ùå Utente riceve notifiche ripetute per stesso appuntamento
- ‚ùå Spam notifiche
- ‚ùå Utente disabilita notifiche
- ‚ùå Perdita fiducia nel sistema

**Probabilit√†**: üü° **MEDIA** (40-60%)
**Severit√†**: üü° **MEDIA** (UX degradata)

**Mitigazione:**
1. ‚úÖ Backend controlla `isNotificationSent()` prima di inviare
2. ‚úÖ Backend marca come sent dopo invio
3. ‚ö†Ô∏è **PROBLEMA**: Se push fallisce, non viene marcata ‚Üí retry infinito
4. üîß **SOLUZIONE**: 
   - Rate limiting: max 1 notifica per appuntamento ogni 24h
   - Exponential backoff su errori
   - Dead letter queue per notifiche fallite

**Raccomandazione**:
- ‚ö†Ô∏è **IMPORTANTE**: Aggiungere rate limiting PRIMA di deploy
- ‚ö†Ô∏è **IMPORTANTE**: Monitorare tasso di delivery push

---

## üü° **RISCHI MEDI**

### **5. RISCHIO: REGRESSIONE LOGICA FILTRI** üü° MEDIO

**Causa Root:**
- Filtro `answered` spostato da SQL a JavaScript
- Logica complessa: NNCF vs vendita normale
- Potenziale bug nella logica di filtro

**Scenario Critico:**
```javascript
// Logica attuale (dopo correzione):
if (appointment.nncf) {
  shouldNotify = appointment.nncfpromptanswered === null || false;
} else {
  shouldNotify = appointment.salepromptanswered === null || false;
}

// BUG POTENZIALE:
// Se appointment.nncf = null (non settato) ‚Üí tratta come vendita normale
// Ma potrebbe essere NNCF non marcato correttamente
```

**Impatto:**
- ‚ùå Banner mostrati quando non dovrebbero
- ‚ùå Banner non mostrati quando dovrebbero
- ‚ùå Inconsistenza dati

**Probabilit√†**: üü¢ **BASSA** (10-20%)
**Severit√†**: üü° **MEDIA** (logica errata)

**Mitigazione:**
1. ‚úÖ Test unitari per logica filtro
2. ‚úÖ Test end-to-end con dati reali
3. ‚ö†Ô∏è **PROBLEMA**: Edge cases non testati
4. üîß **SOLUZIONE**: 
   - Validazione strict: `nncf === true` (non truthy)
   - Logging dettagliato per debug
   - Test con tutti i casi edge

**Raccomandazione**:
- ‚ö†Ô∏è **IMPORTANTE**: Test approfonditi PRIMA di deploy
- ‚ö†Ô∏è **IMPORTANTE**: Logging dettagliato per monitoraggio

---

### **6. RISCHIO: COSTI SUPABASE AUMENTATI** üü° MEDIO

**Causa Root:**
- Query pi√π frequenti (scan frontend ogni 5 min)
- Query pi√π ampie (7 giorni invece di 2 ore)
- Nessun ottimizzazione query

**Calcolo Costi:**
```
Scenario Attuale:
- Query backend: ~20 query/ora √ó 24h = 480 query/giorno
- Query frontend: ~1 query/utente/giorno √ó 10 utenti = 10 query/giorno
- Totale: ~490 query/giorno

Scenario Con Correzione:
- Query backend: ~20 query/ora √ó 24h = 480 query/giorno (stesso)
- Query frontend: ~288 query/utente/giorno √ó 10 utenti = 2,880 query/giorno
- Totale: ~3,360 query/giorno (6.8x aumento)
```

**Impatto:**
- ‚ùå Costi Supabase aumentati del 500-700%
- ‚ùå Possibile superamento quota gratuita
- ‚ùå Rate limiting Supabase

**Probabilit√†**: üü° **MEDIA** (30-50%)
**Severit√†**: üü° **MEDIA** (costi, limitazioni)

**Mitigazione:**
1. ‚úÖ Supabase free tier: 500MB database, 2GB bandwidth
2. ‚ö†Ô∏è **PROBLEMA**: Query count non limitato ma bandwidth s√¨
3. üîß **SOLUZIONE**: 
   - Caching frontend riduce query del 80-90%
   - Incremental refresh invece di full scan
   - Monitorare costi Supabase

**Raccomandazione**:
- ‚ö†Ô∏è **IMPORTANTE**: Implementare caching PRIMA di riabilitare scan
- ‚ö†Ô∏è **IMPORTANTE**: Monitorare costi Supabase settimanalmente

---

### **7. RISCHIO: INCONSISTENZA STATO BANNER** üü° MEDIO

**Causa Root:**
- Frontend e backend hanno logiche separate
- Frontend usa `isBannerAnswered()` (DB appointments)
- Backend usa `isNotificationSent()` (DB push_notifications_sent)
- Due sistemi di tracking non sincronizzati

**Scenario Critico:**
```
T0: Utente risponde banner ‚Üí frontend marca `salepromptanswered=true`
T1: Backend job trova appuntamento ‚Üí controlla `isNotificationSent()` ‚Üí non trovato
T2: Backend invia push (anche se banner gi√† risposto!)
```

**Impatto:**
- ‚ùå Notifiche inviate per banner gi√† risposti
- ‚ùå Inconsistenza stato
- ‚ùå Confusione utente

**Probabilit√†**: üü¢ **BASSA** (20-30%)
**Severit√†**: üü° **MEDIA** (inconsistenza dati)

**Mitigazione:**
1. ‚úÖ Backend controlla `salepromptanswered` / `nncfpromptanswered` nella query
2. ‚ö†Ô∏è **PROBLEMA**: Filtro fatto dopo query, non nella query stessa
3. üîß **SOLUZIONE**: 
   - Backend deve controllare entrambi i flag
   - Sincronizzare sistemi di tracking
   - Unificare logica in un unico posto

**Raccomandazione**:
- ‚ö†Ô∏è **IMPORTANTE**: Backend deve controllare flag `answered` PRIMA di inviare
- ‚ö†Ô∏è **IMPORTANTE**: Test con banner gi√† risposti

---

## üü¢ **RISCHI BASSI**

### **8. RISCHIO: COMPATIBILIT√Ä BROWSER** üü¢ BASSO

**Causa Root:**
- `visibilitychange` API supportata da tutti browser moderni
- `setInterval` standard ma comportamento diverso su mobile

**Impatto:**
- ‚ùå Scan non funziona su browser vecchi
- ‚ùå Comportamento diverso su mobile (background throttling)

**Probabilit√†**: üü¢ **BASSA** (5-10%)
**Severit√†**: üü¢ **BASSA** (limitato a browser vecchi)

**Mitigazione:**
1. ‚úÖ Feature detection
2. ‚úÖ Fallback graceful
3. ‚úÖ Test cross-browser

**Raccomandazione**:
- ‚úÖ **OPZIONALE**: Feature detection e fallback

---

### **9. RISCHIO: SECURITY - RATE LIMITING** üü¢ BASSO

**Causa Root:**
- Scan frontend pu√≤ essere triggerato manualmente
- Nessun rate limiting su endpoint `/api/appointments`

**Impatto:**
- ‚ùå Possibile DoS se utente malintenzionato
- ‚ùå Abuso endpoint

**Probabilit√†**: üü¢ **BASSA** (1-5%)
**Severit√†**: üü¢ **BASSA** (autenticazione gi√† presente)

**Mitigazione:**
1. ‚úÖ Autenticazione richiesta
2. ‚úÖ Rate limiting backend gi√† presente
3. ‚úÖ Logging per audit

**Raccomandazione**:
- ‚úÖ **OPZIONALE**: Aggiungere rate limiting pi√π aggressivo

---

## üìã **CONTROINDICAZIONI SPECIFICHE**

### **1. CONTROINDICAZIONE: UTENTI CON PAGINA SEMPRE APERTA** üü°

**Scenario:**
- Utente tiene pagina aperta 24/7
- Scan ogni 5 minuti = 288 query/giorno
- Carico inutile se utente non attivo

**Impatto:**
- ‚ùå Carico database inutile
- ‚ùå Battery drain (mobile)
- ‚ùå Costi aumentati

**Soluzione:**
- ‚úÖ Scan solo se `!document.hidden`
- ‚úÖ Debounce se utente inattivo > 30 min
- ‚úÖ Pausa scan se tab in background > 1 ora

---

### **2. CONTROINDICAZIONE: APPUNTAMENTI VECCHI (6-7 GIORNI)** üü°

**Scenario:**
- Appuntamenti finiti 6-7 giorni fa
- Backend li processa e invia notifiche
- Utente potrebbe aver gi√† gestito manualmente

**Impatto:**
- ‚ùå Notifiche per eventi vecchi
- ‚ùå Confusione utente
- ‚ùå Spam

**Soluzione:**
- ‚úÖ Limite: max 3 giorni per notifiche automatiche
- ‚úÖ Notifiche solo per appuntamenti < 3 giorni
- ‚úÖ Banner frontend pu√≤ mostrare fino a 7 giorni (OK)

---

### **3. CONTROINDICAZIONE: MULTI-DEVICE UTENTE** üü°

**Scenario:**
- Utente ha app aperta su 2 dispositivi
- Entrambi fanno scan ogni 5 minuti
- Doppio carico database

**Impatto:**
- ‚ùå Carico database doppio
- ‚ùå Possibili duplicati banner

**Soluzione:**
- ‚úÖ Backend gi√† gestisce multi-device (push_subscriptions)
- ‚úÖ Frontend usa `_pending` Set per dedupe
- ‚ö†Ô∏è **PROBLEMA**: `_pending` √® per-sessione, non cross-device
- üîß **SOLUZIONE**: Usare push tracking anche per banner

---

## üéØ **MATRICE RISCHI**

| Rischio | Probabilit√† | Severit√† | Priorit√† | Mitigazione |
|---------|-------------|----------|----------|-------------|
| Duplicati Notifiche | üü° Media | üî¥ Alta | üî¥ CRITICA | Check push tracking PRIMA di banner |
| Performance DB | üü° Media | üî¥ Alta | üî¥ CRITICA | Caching frontend obbligatorio |
| Memory Leak | üü° Media | üü° Media | üü° IMPORTANTE | Cleanup listeners |
| Spam Notifiche | üü° Media | üü° Media | üü° IMPORTANTE | Rate limiting |
| Regressione Logica | üü¢ Bassa | üü° Media | üü° IMPORTANTE | Test approfonditi |
| Costi Supabase | üü° Media | üü° Media | üü° IMPORTANTE | Monitoraggio costi |
| Inconsistenza Stato | üü¢ Bassa | üü° Media | üü° IMPORTANTE | Sincronizzazione sistemi |
| Compatibilit√† Browser | üü¢ Bassa | üü¢ Bassa | üü¢ OPZIONALE | Feature detection |
| Security | üü¢ Bassa | üü¢ Bassa | üü¢ OPZIONALE | Rate limiting |

---

## ‚úÖ **RACCOMANDAZIONI FINALI**

### **PRIMA DI IMPLEMENTARE:**

1. üî¥ **CRITICO**: Implementare caching frontend (5 minuti)
2. üî¥ **CRITICO**: Aggiungere check `pushSent()` PRIMA di `enqueueBanner()`
3. üî¥ **CRITICO**: Cleanup listeners su navigazione SPA
4. üü° **IMPORTANTE**: Rate limiting notifiche (max 1/24h per appuntamento)
5. üü° **IMPORTANTE**: Limite temporale backend (max 3 giorni per notifiche)

### **DURANTE IMPLEMENTAZIONE:**

1. ‚úÖ Test unitari per logica filtro
2. ‚úÖ Test end-to-end con dati reali
3. ‚úÖ Test performance (query count, latenza)
4. ‚úÖ Test multi-device
5. ‚úÖ Test mobile (battery, background throttling)

### **DOPO IMPLEMENTAZIONE:**

1. ‚úÖ Monitoraggio metriche Supabase (query count, bandwidth)
2. ‚úÖ Monitoraggio errori push notifications
3. ‚úÖ Monitoraggio duplicati (log analysis)
4. ‚úÖ Monitoraggio costi Supabase
5. ‚úÖ Feedback utenti (survey, support tickets)

---

## üö® **SCENARI DI ROLLBACK**

### **Scenario 1: Performance Database Degradata**
**Sintomi:**
- Query Supabase > 500ms
- Rate limiting Supabase
- Errori 429 (Too Many Requests)

**Rollback:**
1. Disabilitare scan periodico frontend
2. Ridurre finestra backend a 3 giorni
3. Aumentare intervallo scan a 10 minuti

### **Scenario 2: Duplicati Notifiche**
**Sintomi:**
- Utenti ricevono 2+ notifiche per stesso appuntamento
- Log mostrano duplicati

**Rollback:**
1. Aggiungere distributed lock
2. Disabilitare scan frontend temporaneamente
3. Solo backend gestisce notifiche

### **Scenario 3: Memory Leak**
**Sintomi:**
- Browser crash dopo 1+ ora
- Memory usage crescente
- CPU spike

**Rollback:**
1. Disabilitare scan periodico
2. Solo scan su `visibilitychange`
3. Fix cleanup listeners

---

## üìä **METRICHE DI MONITORAGGIO**

### **Metriche Critiche (Monitorare Ogni Ora):**
- Query count Supabase (target: < 5,000/giorno)
- Latenza query (target: < 200ms p95)
- Push delivery rate (target: > 90%)
- Duplicati notifiche (target: 0%)

### **Metriche Importanti (Monitorare Giornalmente):**
- Costi Supabase (target: < $10/mese)
- Errori push (target: < 5%)
- Banner mostrati vs risposti (target: > 50%)
- User complaints (target: < 1%)

---

**CONCLUSIONE**: 
Le correzioni sono **NECESSARIE** ma richiedono **ATTENZIONE** e **MITIGAZIONI** prima di deploy. Priorit√†: implementare mitigazioni critiche PRIMA di correzioni.

